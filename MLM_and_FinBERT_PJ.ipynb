{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLM_and_FinBERT - PJ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_DKL5q6uf9d"
      },
      "source": [
        "# Masked Language Modeling with BERT to create FinBERT\n",
        "\n",
        "**Author:** [Lalit Vedula and Praveen Joseph]\n",
        "\n",
        "**Date created:** 2021/04/10<br>\n",
        "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the Earnings call data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Uy1aN1uf-H"
      },
      "source": [
        "## GPU Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ3bZYSLvSee",
        "outputId": "c0f8bc63-88e4-4ef7-d7af-74fa0ad03192"
      },
      "source": [
        "# Set up GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3_9rL6Huf-I"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1rBYl74uf-K"
      },
      "source": [
        "## Set-up Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPQFK16uf-M"
      },
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 768\n",
        "    NUM_HEAD = 12  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsNtMDpuf-N"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "We will first download the IMDB data and load into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1REtBmZuf-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de48cb2-5007-4944-cee6-d3363ab9928c"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  27.4M      0  0:00:02  0:00:02 --:--:-- 27.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUL68jkKuf-Q"
      },
      "source": [
        "\n",
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "dDUv3QvF20hw",
        "outputId": "21523032-f4d7-4df3-b591-ac4264c2beba"
      },
      "source": [
        "all_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LES CONVOYEURS ATTENDENT was the first film I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Although the director tried(the filming was ma...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I got to see this film at a preview and was da...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This was the eighth and final Columbia Whistle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This film has the kernel of a really good stor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  LES CONVOYEURS ATTENDENT was the first film I ...          0\n",
              "1  Although the director tried(the filming was ma...          1\n",
              "2  I got to see this film at a preview and was da...          0\n",
              "3  This was the eighth and final Columbia Whistle...          0\n",
              "4  This film has the kernel of a really good stor...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk0tl8ml015U",
        "outputId": "8cab9aee-4ce8-4291-f5bd-3448cfad4ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/W266/Data\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "bert_mlm_finBERT.h5\t     IMDB_reviews.pickle\n",
            "earnings_calls_key_para.csv  transcripts_calls_512tokens.txt\n",
            "IMDB_labels.ob\t\t     transcripts_calls5.csv\n",
            "IMDB_labels.pickle\t     transcripts_calls_all_512tokens.txt\n",
            "IMDB_reviews.ob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VZEpzaxG1zwO",
        "outputId": "47c0bc10-499c-4167-aa9e-b98c1c7cc779"
      },
      "source": [
        "with open('/content/drive/My Drive/W266/Data/transcripts_calls_all_512tokens.txt', 'r', encoding='cp1252') as f:  \n",
        "  lines = f.readlines()\n",
        "\n",
        "all_data = pd.DataFrame(lines, columns=[\"review\"])  \n",
        "len(lines)\n",
        "\n",
        "all_data.head()\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and now i ' d like to introduce you to the hos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>. eps of $ 1 . 06 is up 31 % year - over - yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>growth business . the diagnostic and gen ##omi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>growth , and agile ##nt is in a strong positio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ph ##arm ##a growth rate . we continue to be v...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review\n",
              "0  and now i ' d like to introduce you to the hos...\n",
              "1  . eps of $ 1 . 06 is up 31 % year - over - yea...\n",
              "2  growth business . the diagnostic and gen ##omi...\n",
              "3  growth , and agile ##nt is in a strong positio...\n",
              "4  ph ##arm ##a growth rate . we continue to be v..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUW617G3pjPR",
        "outputId": "c79e4e57-8275-4026-8d2f-9e9fa850b75b"
      },
      "source": [
        "#load the financial phrasebank\n",
        "import random\n",
        "import itertools\n",
        "s2 = []\n",
        "l2 = []\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/W266/FinancialPhraseBank-v1.0/Sentences_50Agree.txt', 'r', encoding='cp1252') as f:  \n",
        "  lines = f.readlines()\n",
        "  random.shuffle(lines)\n",
        "  for line in lines:\n",
        "    line_split = line.split('@')\n",
        "    if line_split[-1][:-1] != 'neutral':\n",
        "      s2.append(line_split[:-1])\n",
        "      l2.append(line_split[-1][:-1])    \n",
        "\n",
        "sentences = list(itertools.chain(*s2))\n",
        "labels = []\n",
        "\n",
        "for label in l2:\n",
        "  if label == 'positive':\n",
        "    labels.append(0)\n",
        "  elif label == 'negative':\n",
        "    labels.append(1)\n",
        "\n",
        "\n",
        "\n",
        "print('len of senteces:',len(sentences))\n",
        "print('len of labels:',len(labels))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of senteces: 1967\n",
            "len of labels: 1967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHbDhGjhuf-S"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
        "It transforms a batch of strings into either\n",
        "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
        "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
        "2.  The `encode` function encodes raw text into integer token ids.\n",
        "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
        "It masks 15% of all input tokens in each sequence at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvX8Rta1uf-U"
      },
      "source": [
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgaUL1N3z48x"
      },
      "source": [
        "# Train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Jl1z3yz5Lf"
      },
      "source": [
        "\n",
        "\n",
        "#IMDB\n",
        "# We have 25000 examples for training\n",
        "# x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "# y_train = train_df.sentiment.values\n",
        "\n",
        "# # We have 25000 examples for testing\n",
        "# x_test = encode(test_df.review.values)\n",
        "# y_test = test_df.sentiment.values\n",
        "\n",
        "\n",
        "#Financial Phrasebank\n",
        "train_sentences_length = int(0.9*len(sentences))\n",
        "\n",
        "\n",
        "x_train = encode(sentences[:train_sentences_length])\n",
        "y_train = labels[:train_sentences_length]\n",
        "\n",
        "x_test = encode(sentences[train_sentences_length:])\n",
        "y_test = labels[train_sentences_length:]\n",
        "\n",
        "\n",
        "\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(100)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (sentences[train_sentences_length:], y_test)\n",
        ").batch(config.BATCH_SIZE)\n",
        "\n",
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOKSqmwBuf-i"
      },
      "source": [
        "## Create BERT model (Pretraining Model) for masked language modeling\n",
        "\n",
        "We will create a BERT-like pretraining model architecture\n",
        "using the `MultiHeadAttention` layer.\n",
        "It will take token ids as inputs (including masked tokens)\n",
        "and it will predict the correct ids for the masked input tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbM_i23uuf-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9cf44d-6033-494d-a54a-8ba99b8b5f29"
      },
      "source": [
        "\n",
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "# sample_tokens = vectorize_layer([\"provide the opportunity for [MASK] and individuals to step up and [MASK] strength\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 256, 768)     23040000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 256, 768)     0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/multiheadattention (M (None, 256, 768)     2362368     tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_dropout (Dropout) (None, 256, 768)     0           encoder_0/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 256, 768)     0           tf.__operators__.add[0][0]       \n",
            "                                                                 encoder_0/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_layernormalizatio (None, 256, 768)     1536        tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn (Sequential)      (None, 256, 768)     197504      encoder_0/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_dropout (Dropout) (None, 256, 768)     0           encoder_0/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, 256, 768)     0           encoder_0/att_layernormalization[\n",
            "                                                                 encoder_0/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_layernormalizatio (None, 256, 768)     1536        tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mlm_cls (Dense)                 (None, 256, 30000)   23070000    encoder_0/ffn_layernormalization[\n",
            "==================================================================================================\n",
            "Total params: 48,672,944\n",
            "Trainable params: 48,672,944\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13yKDvZxuf-x"
      },
      "source": [
        "## Train and Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXiXhGnBuf_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff28588e-3cc9-4ade-bc13-391a0b1c4fca"
      },
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=2, callbacks=[generator_callback])\n",
        "# model_name = \"bert_mlm_imdb.h5\"\n",
        "model_name = \"bert_mlm_finBERT.h5\"\n",
        "\n",
        "bert_masked_model.save(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1264/1264 [==============================] - 400s 314ms/step - loss: 6.7184\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'and',\n",
            " 'prediction': 'i have watched this and and it was awesome',\n",
            " 'probability': 0.040390972}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'we',\n",
            " 'prediction': 'i have watched this we and it was awesome',\n",
            " 'probability': 0.035694316}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'to',\n",
            " 'prediction': 'i have watched this to and it was awesome',\n",
            " 'probability': 0.031775672}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.030505795}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'that',\n",
            " 'prediction': 'i have watched this that and it was awesome',\n",
            " 'probability': 0.029187808}\n",
            "Epoch 2/2\n",
            " 525/1264 [===========>..................] - ETA: 3:49 - loss: 6.5579"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTD8rGHruf_D"
      },
      "source": [
        "## Fine-tune a sentiment classification model\n",
        "\n",
        "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
        "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
        "pretrained BERT features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnyqSh-uf_E"
      },
      "source": [
        "# Load pretrained bert model\n",
        "mlm_model = keras.models.load_model(\n",
        "    model_name, custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "pretrained_bert_model = tf.keras.Model(\n",
        "    mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
        ")\n",
        "\n",
        "# Freeze it\n",
        "pretrained_bert_model.trainable = False\n",
        "\n",
        "\n",
        "def create_classifier_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "    sequence_output = pretrained_bert_model(inputs)\n",
        "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    classifer_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return classifer_model\n",
        "\n",
        "\n",
        "classifer_model = create_classifier_bert_model()\n",
        "classifer_model.summary()\n",
        "\n",
        "# Train the classifier with frozen BERT stage\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")\n",
        "\n",
        "# Unfreeze the BERT model for fine-tuning\n",
        "pretrained_bert_model.trainable = True\n",
        "optimizer = keras.optimizers.Adam()\n",
        "classifer_model.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "classifer_model.fit(\n",
        "    train_classifier_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_classifier_ds,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxw6jS9Guf_G"
      },
      "source": [
        "## Create an end-to-end model and evaluate it\n",
        "\n",
        "When you want to deploy a model, it's best if it already includes its preprocessing\n",
        "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
        "production environment. Let's create an end-to-end model that incorporates\n",
        "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
        "as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAD3chgIuf_H"
      },
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "\n",
        "def get_end_to_end(model):\n",
        "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
        "    indices = vectorize_layer(inputs_string)\n",
        "    outputs = model(indices)\n",
        "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    end_to_end_model.compile(\n",
        "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return end_to_end_model\n",
        "\n",
        "\n",
        "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
        "end_to_end_classification_model.evaluate(test_raw_classifier_ds)\n",
        "\n",
        "time.time() - t0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}