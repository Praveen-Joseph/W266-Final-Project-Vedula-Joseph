{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoN0eX8iGlya"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v7nQhRhcqSVp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = '/Users/PJHome/Desktop/W266-Final-Project-Vedula-Joseph/Data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mBERT Notebooks\u001b[m\u001b[m                     README.md\r\n",
      "\u001b[34mData\u001b[m\u001b[m                               W266_Final_Project_PJ_v2.ipynb\r\n",
      "\u001b[34mFinancialPhraseBank-v1.0\u001b[m\u001b[m           W266_Final_Project_V1.ipynb\r\n",
      "IMDB Sentiment classifier.ipynb    get_test_data.ipynb\r\n",
      "IMDB_Sentiment_classifier_V2.ipynb \u001b[34mhuggingface_notebook\u001b[m\u001b[m\r\n",
      "LICENSE\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of texts is: 25000\n",
      "No of labels is: 25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_reviews = []\n",
    "labels = labels[0:25000]\n",
    "imdb_reviews = texts + labels\n",
    "\n",
    "print(\"No of texts is:\", len(texts))\n",
    "print(\"No of labels is:\", len(labels))\n",
    "type(imdb_reviews)\n",
    "\n",
    "sum(labels[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/PJHome/Desktop/W266-Final-Project-Vedula-Joseph/Data/IMDB_reviews.pickle', 'wb') as fp:\n",
    "    pickle.dump(texts, fp)\n",
    "    \n",
    "with open('/Users/PJHome/Desktop/W266-Final-Project-Vedula-Joseph/Data/IMDB_labels.pickle', 'wb') as fp:\n",
    "    pickle.dump(labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 2 reviews are  [\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", 'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.']\n",
      "The first 2 labels are  [0, 0]\n"
     ]
    }
   ],
   "source": [
    "with open ('/Users/PJHome/Desktop/W266-Final-Project-Vedula-Joseph/Data/IMDB_reviews.pickle', 'rb') as fp:\n",
    "    IMDB_reviews = pickle.load(fp)\n",
    "with open ('/Users/PJHome/Desktop/W266-Final-Project-Vedula-Joseph/Data/IMDB_labels.pickle', 'rb') as fp:\n",
    "    IMDB_labels = pickle.load(fp)\n",
    "    \n",
    "print(\"The first 2 reviews are \", IMDB_reviews[0:2])\n",
    "print(\"The first 2 labels are \", IMDB_labels[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aT56gKhvObv"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,64) # 1000 = # of possible tokens and 64 = Embedding Dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qTWoLA1Glyp",
    "outputId": "18323196-5990-4eab-89b1-952b0301b117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000 # We choose only the top 10K words in the IMDB dataset\n",
    "maxlen = 100 # MSL of 20, all sequences > 20 are truncated\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "\n",
    "train_data = preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r33rk0zcGlyr",
    "outputId": "138b1ebb-c2dd-464b-fe75-7542a25ab063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 80,801\n",
      "Trainable params: 80,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 0.6760 - acc: 0.5753 - val_loss: 0.4873 - val_acc: 0.7914\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4099 - acc: 0.8416 - val_loss: 0.3543 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2944 - acc: 0.8828 - val_loss: 0.3280 - val_acc: 0.8556\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2476 - acc: 0.9028 - val_loss: 0.3224 - val_acc: 0.8592\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.2238 - acc: 0.9112 - val_loss: 0.3247 - val_acc: 0.8584\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1993 - acc: 0.9234 - val_loss: 0.3303 - val_acc: 0.8594\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1859 - acc: 0.9271 - val_loss: 0.3378 - val_acc: 0.8590\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1675 - acc: 0.9384 - val_loss: 0.3442 - val_acc: 0.8576\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1450 - acc: 0.9490 - val_loss: 0.3544 - val_acc: 0.8548\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1269 - acc: 0.9569 - val_loss: 0.3654 - val_acc: 0.8532\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras import layers\n",
    "# from keras import Input\n",
    "\n",
    "# seq_model = Sequential()\n",
    "# seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "# seq_model.add(layers.Dense(32, activation='relu'))\n",
    "# seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# input_tensor = Input(shape=(20,))\n",
    "# x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "# x = layers.Dense(32, activation='relu')(x)\n",
    "# output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(32,activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZhO6u5iGlyu",
    "outputId": "bcab24cd-e4de-4a46-e826-844793ebd326"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.0308723449707"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYwPSoCCHX45"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB Sentiment classifier V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
